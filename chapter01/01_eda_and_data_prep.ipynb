{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470df438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set up paths\n",
    "ROOT_DIR = Path(\"..\")\n",
    "DATA_DIR = ROOT_DIR\n",
    "OUTPUT_DIR = Path(\".\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6e2c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples dataset shape: (2621288, 9)\n",
      "\n",
      "Columns: ['example_id', 'query', 'query_id', 'product_id', 'product_locale', 'esci_label', 'small_version', 'large_version', 'split']\n",
      "\n",
      "ESCI label distribution:\n",
      "E    1708158\n",
      "S     574313\n",
      "I     263165\n",
      "C      75652\n",
      "Name: esci_label, dtype: int64\n",
      "\n",
      "Split distribution:\n",
      "train    1983272\n",
      "test      638016\n",
      "Name: split, dtype: int64\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B000MOO21W</td>\n",
       "      <td>us</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07X3Y6B1V</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07WDM7MQQ</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07RH6Z8KW</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07QJ7WYFQ</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id           query  query_id  product_id product_locale esci_label  \\\n",
       "0           0   revent 80 cfm         0  B000MOO21W             us          I   \n",
       "1           1   revent 80 cfm         0  B07X3Y6B1V             us          E   \n",
       "2           2   revent 80 cfm         0  B07WDM7MQQ             us          E   \n",
       "3           3   revent 80 cfm         0  B07RH6Z8KW             us          E   \n",
       "4           4   revent 80 cfm         0  B07QJ7WYFQ             us          E   \n",
       "\n",
       "   small_version  large_version  split  \n",
       "0              0              1  train  \n",
       "1              0              1  train  \n",
       "2              0              1  train  \n",
       "3              0              1  train  \n",
       "4              0              1  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the examples dataset (queries and relevance labels)\n",
    "examples_df = pd.read_parquet(DATA_DIR / \"shopping_queries_dataset_examples.parquet\")\n",
    "print(f\"Examples dataset shape: {examples_df.shape}\")\n",
    "print(f\"\\nColumns: {examples_df.columns.tolist()}\")\n",
    "print(f\"\\nESCI label distribution:\")\n",
    "print(examples_df['esci_label'].value_counts())\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(examples_df['split'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "examples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e03c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products dataset shape: (1814924, 7)\n",
      "\n",
      "Columns: ['product_id', 'product_title', 'product_description', 'product_bullet_point', 'product_brand', 'product_color', 'product_locale']\n",
      "\n",
      "Missing values:\n",
      "product_id                   0\n",
      "product_title                0\n",
      "product_description     877799\n",
      "product_bullet_point    304116\n",
      "product_brand           142756\n",
      "product_color           691001\n",
      "product_locale               0\n",
      "dtype: int64\n",
      "\n",
      "Product locales:\n",
      "us    1215854\n",
      "jp     339059\n",
      "es     260011\n",
      "Name: product_locale, dtype: int64\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "      <th>product_locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B079VKKJN7</td>\n",
       "      <td>11 Degrees de los Hombres Playera con Logo, Ne...</td>\n",
       "      <td>Esta playera con el logo de la marca Carrier d...</td>\n",
       "      <td>11 Degrees Negro Playera con logo\\nA estrenar ...</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Negro</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B079Y9VRKS</td>\n",
       "      <td>Camiseta Eleven Degrees Core TS White (M)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07DP4LM9H</td>\n",
       "      <td>11 Degrees de los Hombres Core Pull Over Hoodi...</td>\n",
       "      <td>La sudadera con capucha Core Pull Over de 11 G...</td>\n",
       "      <td>11 Degrees Azul Core Pull Over Hoodie\\nA estre...</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Azul</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07G37B9HP</td>\n",
       "      <td>11 Degrees Poli Panel Track Pant XL Black</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07LCTGDHY</td>\n",
       "      <td>11 Degrees Gorra Trucker Negro OSFA (Talla úni...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Negro (</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                      product_title  \\\n",
       "0  B079VKKJN7  11 Degrees de los Hombres Playera con Logo, Ne...   \n",
       "1  B079Y9VRKS          Camiseta Eleven Degrees Core TS White (M)   \n",
       "2  B07DP4LM9H  11 Degrees de los Hombres Core Pull Over Hoodi...   \n",
       "3  B07G37B9HP          11 Degrees Poli Panel Track Pant XL Black   \n",
       "4  B07LCTGDHY  11 Degrees Gorra Trucker Negro OSFA (Talla úni...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Esta playera con el logo de la marca Carrier d...   \n",
       "1                                               None   \n",
       "2  La sudadera con capucha Core Pull Over de 11 G...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                product_bullet_point product_brand  \\\n",
       "0  11 Degrees Negro Playera con logo\\nA estrenar ...    11 Degrees   \n",
       "1                                               None    11 Degrees   \n",
       "2  11 Degrees Azul Core Pull Over Hoodie\\nA estre...    11 Degrees   \n",
       "3                                               None    11 Degrees   \n",
       "4                                               None    11 Degrees   \n",
       "\n",
       "  product_color product_locale  \n",
       "0         Negro             es  \n",
       "1        Blanco             es  \n",
       "2          Azul             es  \n",
       "3          None             es  \n",
       "4       Negro (             es  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the products dataset\n",
    "products_df = pd.read_parquet(DATA_DIR / \"shopping_queries_dataset_products.parquet\")\n",
    "print(f\"Products dataset shape: {products_df.shape}\")\n",
    "print(f\"\\nColumns: {products_df.columns.tolist()}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(products_df.isnull().sum())\n",
    "print(f\"\\nProduct locales:\")\n",
    "print(products_df['product_locale'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b64a2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA EXPLORATION\n",
      "================================================================================\n",
      "\n",
      "1. Products Dataset:\n",
      "   Total products: 1,814,924\n",
      "   Unique product IDs: 1,802,772\n",
      "   Products with descriptions: 937,125 (51.6%)\n",
      "   Products with bullet points: 1,510,808 (83.2%)\n",
      "\n",
      "2. Examples Dataset:\n",
      "   Total examples: 2,621,288\n",
      "   Unique queries: 130,193\n",
      "   Unique products in examples: 1,802,772\n",
      "   Products in both datasets: 1,802,772 (100.0% of examples)\n",
      "\n",
      "3. ESCI Label Distribution:\n",
      "E    65.164835\n",
      "S    21.909573\n",
      "I    10.039530\n",
      "C     2.886062\n",
      "Name: esci_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration and Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. Products Dataset:\")\n",
    "print(f\"   Total products: {len(products_df):,}\")\n",
    "print(f\"   Unique product IDs: {products_df['product_id'].nunique():,}\")\n",
    "print(f\"   Products with descriptions: {products_df['product_description'].notna().sum():,} ({products_df['product_description'].notna().sum()/len(products_df)*100:.1f}%)\")\n",
    "print(f\"   Products with bullet points: {products_df['product_bullet_point'].notna().sum():,} ({products_df['product_bullet_point'].notna().sum()/len(products_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. Examples Dataset:\")\n",
    "print(f\"   Total examples: {len(examples_df):,}\")\n",
    "print(f\"   Unique queries: {examples_df['query'].nunique():,}\")\n",
    "print(f\"   Unique products in examples: {examples_df['product_id'].nunique():,}\")\n",
    "\n",
    "# Check overlap\n",
    "products_in_examples = set(examples_df['product_id'].unique())\n",
    "products_in_catalog = set(products_df['product_id'].unique())\n",
    "overlap = products_in_examples.intersection(products_in_catalog)\n",
    "print(f\"   Products in both datasets: {len(overlap):,} ({len(overlap)/len(products_in_examples)*100:.1f}% of examples)\")\n",
    "\n",
    "print(f\"\\n3. ESCI Label Distribution:\")\n",
    "print(examples_df['esci_label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6217d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING AND PREPARATION\n",
      "================================================================================\n",
      "\n",
      "1. Filtering products to those in examples dataset...\n",
      "   Products after filtering: 1,814,924\n",
      "\n",
      "2. Creating combined text field for products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1814924/1814924 [01:45<00:00, 17234.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text length statistics:\n",
      "     Mean: 1037 characters\n",
      "     Median: 710 characters\n",
      "     Min: 1 characters\n",
      "     Max: 8803 characters\n",
      "     Products with empty text: 0\n",
      "\n",
      "   Final product count: 1,814,924\n",
      "\n",
      "3. Sample product text:\n",
      "--------------------------------------------------------------------------------\n",
      "Product ID: B079VKKJN7\n",
      "Title: 11 Degrees de los Hombres Playera con Logo, Negro, L\n",
      "\n",
      "Combined Text (first 200 chars):\n",
      "11 Degrees de los Hombres Playera con Logo, Negro, L Esta playera con el logo de la marca Carrier de 11 Degrees viene en negro, con el logo de la marca en el pecho y un pequeño texto en la parte poste...\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning and Preparation\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING AND PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter products to only those in the examples dataset (for consistency)\n",
    "print(f\"\\n1. Filtering products to those in examples dataset...\")\n",
    "products_df_clean = products_df[products_df['product_id'].isin(products_in_examples)].copy()\n",
    "print(f\"   Products after filtering: {len(products_df_clean):,}\")\n",
    "\n",
    "# Handle missing values and create combined text field\n",
    "print(f\"\\n2. Creating combined text field for products...\")\n",
    "\n",
    "def create_product_text(row):\n",
    "    \"\"\"Combine product fields into a single text for embedding\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Title (always present)\n",
    "    if pd.notna(row['product_title']):\n",
    "        parts.append(str(row['product_title']).strip())\n",
    "    \n",
    "    # Description\n",
    "    if pd.notna(row['product_description']):\n",
    "        desc = str(row['product_description']).strip()\n",
    "        if desc and desc.lower() != 'none':\n",
    "            parts.append(desc)\n",
    "    \n",
    "    # Bullet points\n",
    "    if pd.notna(row['product_bullet_point']):\n",
    "        bullets = str(row['product_bullet_point']).strip()\n",
    "        if bullets and bullets.lower() != 'none':\n",
    "            parts.append(bullets)\n",
    "    \n",
    "    # Brand\n",
    "    if pd.notna(row['product_brand']):\n",
    "        brand = str(row['product_brand']).strip()\n",
    "        if brand and brand.lower() != 'none':\n",
    "            parts.append(f\"Brand: {brand}\")\n",
    "    \n",
    "    # Color\n",
    "    if pd.notna(row['product_color']):\n",
    "        color = str(row['product_color']).strip()\n",
    "        if color and color.lower() != 'none':\n",
    "            parts.append(f\"Color: {color}\")\n",
    "    \n",
    "    # Join all parts\n",
    "    combined = \" \".join(parts)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    combined = \" \".join(combined.split())\n",
    "    \n",
    "    return combined if combined else \"No description available\"\n",
    "\n",
    "products_df_clean['product_text'] = products_df_clean.progress_apply(create_product_text, axis=1)\n",
    "\n",
    "# Check text length statistics\n",
    "text_lengths = products_df_clean['product_text'].str.len()\n",
    "print(f\"   Text length statistics:\")\n",
    "print(f\"     Mean: {text_lengths.mean():.0f} characters\")\n",
    "print(f\"     Median: {text_lengths.median():.0f} characters\")\n",
    "print(f\"     Min: {text_lengths.min()} characters\")\n",
    "print(f\"     Max: {text_lengths.max()} characters\")\n",
    "print(f\"     Products with empty text: {(products_df_clean['product_text'].str.len() == 0).sum()}\")\n",
    "\n",
    "# Remove products with empty text\n",
    "products_df_clean = products_df_clean[products_df_clean['product_text'].str.len() > 0].copy()\n",
    "print(f\"\\n   Final product count: {len(products_df_clean):,}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n3. Sample product text:\")\n",
    "print(\"-\" * 80)\n",
    "sample_idx = 0\n",
    "sample_product = products_df_clean.iloc[sample_idx]\n",
    "print(f\"Product ID: {sample_product['product_id']}\")\n",
    "print(f\"Title: {sample_product['product_title']}\")\n",
    "print(f\"\\nCombined Text (first 200 chars):\")\n",
    "print(sample_product['product_text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e055f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING GROUND TRUTH FOR EVALUATION\n",
      "================================================================================\n",
      "Examples after filtering: 2,621,288\n",
      "\n",
      "Creating ground truth labels...\n",
      "ESCI label meanings:\n",
      "  E = Exact (relevant)\n",
      "  S = Substitute (relevant)\n",
      "  C = Complement (relevant)\n",
      "  I = Irrelevant (not relevant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 130652/130652 [00:49<00:00, 2632.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth created for 130,193 unique queries\n",
      "Average relevant products per query: 18.0\n",
      "Ground truth saved to ground_truth.pkl\n"
     ]
    }
   ],
   "source": [
    "# Prepare ground truth for evaluation\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARING GROUND TRUTH FOR EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter examples to only include products that exist in our cleaned catalog\n",
    "examples_df_clean = examples_df[examples_df['product_id'].isin(products_df_clean['product_id'])].copy()\n",
    "print(f\"Examples after filtering: {len(examples_df_clean):,}\")\n",
    "\n",
    "# Create ground truth: for each query, collect relevant product IDs\n",
    "# We'll consider 'E' (Exact), 'S' (Substitute), and 'C' (Complement) as relevant\n",
    "# 'I' (Irrelevant) is not relevant\n",
    "print(f\"\\nCreating ground truth labels...\")\n",
    "print(f\"ESCI label meanings:\")\n",
    "print(f\"  E = Exact (relevant)\")\n",
    "print(f\"  S = Substitute (relevant)\")\n",
    "print(f\"  C = Complement (relevant)\")\n",
    "print(f\"  I = Irrelevant (not relevant)\")\n",
    "\n",
    "# Create ground truth dictionary: query -> set of relevant product IDs\n",
    "ground_truth = {}\n",
    "for query_id, group in tqdm(examples_df_clean.groupby('query_id'), total=len(examples_df_clean['query_id'].unique())):\n",
    "    query_text = group['query'].iloc[0]\n",
    "    # Get relevant products (E, S, C)\n",
    "    relevant_products = set(\n",
    "        group[group['esci_label'].isin(['E', 'S', 'C'])]['product_id'].tolist()\n",
    "    )\n",
    "    if len(relevant_products) > 0:\n",
    "        ground_truth[query_text] = relevant_products\n",
    "\n",
    "print(f\"\\nGround truth created for {len(ground_truth):,} unique queries\")\n",
    "print(f\"Average relevant products per query: {np.mean([len(v) for v in ground_truth.values()]):.1f}\")\n",
    "\n",
    "# Save ground truth for later use\n",
    "ground_truth_file = OUTPUT_DIR / \"ground_truth.pkl\"\n",
    "with open(ground_truth_file, 'wb') as f:\n",
    "    pickle.dump(ground_truth, f)\n",
    "print(f\"Ground truth saved to {ground_truth_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1445b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING EMBEDDING MODEL\n",
      "================================================================================\n",
      "Loading sentence transformer model: all-MiniLM-L6-v2\n",
      "Model loaded successfully!\n",
      "Embedding dimension: 384\n",
      "Model max sequence length: 256\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embedding Model\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING EMBEDDING MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use a lightweight but effective model\n",
    "model_name = \"all-MiniLM-L6-v2\"  # 384 dimensions, fast and efficient\n",
    "print(f\"Loading sentence transformer model: {model_name}\")\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Model max sequence length: {embedding_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d017a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING EMBEDDINGS AND BUILDING FAISS INDICES\n",
      "================================================================================\n",
      "Total products: 1,814,924\n",
      "Processing in chunks of 2,000 products (directly from dataframe)\n",
      "Embedding batch size: 32\n",
      "\n",
      "Initializing FAISS indices...\n",
      "\n",
      "Processing embeddings and building indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing chunks:   0%|                                | 0/908 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings and Build FAISS Indices Incrementally\n",
    "# This approach processes directly from dataframe to avoid memory issues\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING EMBEDDINGS AND BUILDING FAISS INDICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import gc  # For memory cleanup\n",
    "\n",
    "# Process directly from dataframe - no large lists in memory\n",
    "n_products = len(products_df_clean)\n",
    "chunk_size = 2000  # Smaller chunks for better memory management\n",
    "batch_size = 32  # Smaller batch size for embedding model\n",
    "\n",
    "print(f\"Total products: {n_products:,}\")\n",
    "print(f\"Processing in chunks of {chunk_size:,} products (directly from dataframe)\")\n",
    "print(f\"Embedding batch size: {batch_size}\")\n",
    "\n",
    "# Initialize FAISS indices\n",
    "print(\"\\nInitializing FAISS indices...\")\n",
    "flat_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# For IVF, we need to train first\n",
    "nlist = min(100, n_products // 10)\n",
    "quantizer = faiss.IndexFlatL2(embedding_dim)\n",
    "ivf_index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
    "ivf_trained = False\n",
    "\n",
    "# HNSW can be built incrementally\n",
    "m = 32\n",
    "ef_construction = 200\n",
    "ef_search = 50\n",
    "hnsw_index = faiss.IndexHNSWFlat(embedding_dim, m)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "\n",
    "# Process dataframe in chunks directly\n",
    "print(f\"\\nProcessing embeddings and building indices...\")\n",
    "n_chunks = (n_products + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Store product IDs as we process (needed for later)\n",
    "product_ids = []\n",
    "\n",
    "for chunk_idx in tqdm(range(n_chunks), desc=\"Processing chunks\"):\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = min((chunk_idx + 1) * chunk_size, n_products)\n",
    "    \n",
    "    # Get chunk directly from dataframe (only loads this chunk into memory)\n",
    "    chunk_df = products_df_clean.iloc[start_idx:end_idx].copy()\n",
    "    chunk_texts = chunk_df['product_text'].tolist()  # Small list, only for this chunk\n",
    "    chunk_ids = chunk_df['product_id'].tolist()\n",
    "    product_ids.extend(chunk_ids)  # Store IDs for later use\n",
    "    \n",
    "    # Generate embeddings for this chunk\n",
    "    chunk_embeddings = embedding_model.encode(\n",
    "        chunk_texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        device='cpu'\n",
    "    ).astype('float32')\n",
    "    \n",
    "    # Add to Flat index\n",
    "    flat_index.add(chunk_embeddings)\n",
    "    \n",
    "    # For IVF: train on first chunk, then add incrementally\n",
    "    if not ivf_trained:\n",
    "        print(f\"   Training IVF index on first chunk...\")\n",
    "        ivf_index.train(chunk_embeddings)\n",
    "        ivf_trained = True\n",
    "    ivf_index.add(chunk_embeddings)\n",
    "    \n",
    "    # Add to HNSW index\n",
    "    hnsw_index.add(chunk_embeddings)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del chunk_df, chunk_texts, chunk_embeddings\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print progress every 20 chunks\n",
    "    if (chunk_idx + 1) % 20 == 0:\n",
    "        print(f\"   Processed {end_idx:,} / {n_products:,} products ({end_idx/n_products*100:.1f}%)\")\n",
    "\n",
    "# Set IVF nprobe\n",
    "ivf_index.nprobe = 10\n",
    "\n",
    "print(f\"\\nAll embeddings generated and indices built!\")\n",
    "print(f\"  Flat index: {flat_index.ntotal:,} vectors\")\n",
    "print(f\"  IVF index: {ivf_index.ntotal:,} vectors (nlist={nlist}, nprobe=10)\")\n",
    "print(f\"  HNSW index: {hnsw_index.ntotal:,} vectors (m={m})\")\n",
    "print(f\"  Product IDs stored: {len(product_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"INDEX STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. FlatL2 Index (Exact Search):\")\n",
    "print(f\"   Vectors: {flat_index.ntotal:,}\")\n",
    "print(f\"   Index size: {flat_index.ntotal * embedding_dim * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(f\"\\n2. IVF Index (Approximate Search):\")\n",
    "print(f\"   Vectors: {ivf_index.ntotal:,}\")\n",
    "print(f\"   Parameters: nlist={nlist}, nprobe=10\")\n",
    "\n",
    "print(f\"\\n3. HNSW Index (Approximate Search):\")\n",
    "print(f\"   Vectors: {hnsw_index.ntotal:,}\")\n",
    "print(f\"   Parameters: m={m}, ef_construction={ef_construction}, ef_search={ef_search}\")\n",
    "\n",
    "print(\"\\n✓ All indices ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c536e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Indices\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING THE INDICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a sample query from the examples\n",
    "test_query = list(ground_truth.keys())[0]\n",
    "print(f\"\\nTest query: '{test_query}'\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = embedding_model.encode(\n",
    "    [test_query],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")[0].astype('float32')\n",
    "\n",
    "print(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "\n",
    "# Search in each index\n",
    "k = 10\n",
    "print(f\"\\nSearching for top {k} results in each index...\")\n",
    "\n",
    "# Flat index\n",
    "import time\n",
    "start = time.time()\n",
    "flat_distances, flat_indices = flat_index.search(query_embedding.reshape(1, -1), k)\n",
    "flat_time = time.time() - start\n",
    "print(f\"\\n1. Flat Index:\")\n",
    "print(f\"   Search time: {flat_time*1000:.2f}ms\")\n",
    "print(f\"   Top {k} results:\")\n",
    "for i, (dist, idx) in enumerate(zip(flat_distances[0], flat_indices[0]), 1):\n",
    "    product_id = product_ids[idx]\n",
    "    product_title = products_df_clean[products_df_clean['product_id'] == product_id]['product_title'].iloc[0]\n",
    "    print(f\"     {i}. [{product_id}] {product_title[:60]}... (distance: {dist:.4f})\")\n",
    "\n",
    "# IVF index\n",
    "start = time.time()\n",
    "ivf_distances, ivf_indices = ivf_index.search(query_embedding.reshape(1, -1), k)\n",
    "ivf_time = time.time() - start\n",
    "print(f\"\\n2. IVF Index:\")\n",
    "print(f\"   Search time: {ivf_time*1000:.2f}ms\")\n",
    "print(f\"   Top {k} results:\")\n",
    "for i, (dist, idx) in enumerate(zip(ivf_distances[0], ivf_indices[0]), 1):\n",
    "    product_id = product_ids[idx]\n",
    "    product_title = products_df_clean[products_df_clean['product_id'] == product_id]['product_title'].iloc[0]\n",
    "    print(f\"     {i}. [{product_id}] {product_title[:60]}... (distance: {dist:.4f})\")\n",
    "\n",
    "# HNSW index\n",
    "start = time.time()\n",
    "hnsw_distances, hnsw_indices = hnsw_index.search(query_embedding.reshape(1, -1), k)\n",
    "hnsw_time = time.time() - start\n",
    "print(f\"\\n3. HNSW Index:\")\n",
    "print(f\"   Search time: {hnsw_time*1000:.2f}ms\")\n",
    "print(f\"   Top {k} results:\")\n",
    "for i, (dist, idx) in enumerate(zip(hnsw_distances[0], hnsw_indices[0]), 1):\n",
    "    product_id = product_ids[idx]\n",
    "    product_title = products_df_clean[products_df_clean['product_id'] == product_id]['product_title'].iloc[0]\n",
    "    print(f\"     {i}. [{product_id}] {product_title[:60]}... (distance: {dist:.4f})\")\n",
    "\n",
    "print(f\"\\nSpeed comparison:\")\n",
    "print(f\"  Flat:  {flat_time*1000:.2f}ms\")\n",
    "print(f\"  IVF:   {ivf_time*1000:.2f}ms ({flat_time/ivf_time:.2f}x faster)\")\n",
    "print(f\"  HNSW:  {hnsw_time*1000:.2f}ms ({flat_time/hnsw_time:.2f}x faster)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Everything to Disk\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING TO DISK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = OUTPUT_DIR / \"data\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Save FAISS indices\n",
    "print(\"\\n1. Saving FAISS indices...\")\n",
    "\n",
    "# Flat index\n",
    "flat_index_file = output_dir / \"faiss_index_flat.bin\"\n",
    "faiss.write_index(flat_index, str(flat_index_file))\n",
    "print(f\"   Flat index saved to: {flat_index_file}\")\n",
    "\n",
    "# IVF index\n",
    "ivf_index_file = output_dir / \"faiss_index_ivf.bin\"\n",
    "faiss.write_index(ivf_index, str(ivf_index_file))\n",
    "print(f\"   IVF index saved to: {ivf_index_file}\")\n",
    "\n",
    "# HNSW index\n",
    "hnsw_index_file = output_dir / \"faiss_index_hnsw.bin\"\n",
    "faiss.write_index(hnsw_index, str(hnsw_index_file))\n",
    "print(f\"   HNSW index saved to: {hnsw_index_file}\")\n",
    "\n",
    "# 2. Save product IDs mapping\n",
    "print(\"\\n2. Saving product IDs mapping...\")\n",
    "product_ids_file = output_dir / \"product_ids.pkl\"\n",
    "with open(product_ids_file, 'wb') as f:\n",
    "    pickle.dump(product_ids, f)\n",
    "print(f\"   Product IDs saved to: {product_ids_file}\")\n",
    "\n",
    "# 3. Save cleaned products dataframe\n",
    "print(\"\\n3. Saving cleaned products dataframe...\")\n",
    "products_file = output_dir / \"products_clean.parquet\"\n",
    "products_df_clean.to_parquet(products_file, index=False)\n",
    "print(f\"   Products dataframe saved to: {products_file}\")\n",
    "\n",
    "# 4. Save metadata\n",
    "print(\"\\n4. Saving metadata...\")\n",
    "metadata = {\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'model_name': model_name,\n",
    "    'n_products': len(product_ids),\n",
    "    'index_types': ['flat', 'ivf', 'hnsw'],\n",
    "    'ivf_params': {'nlist': nlist, 'nprobe': nprobe},\n",
    "    'hnsw_params': {'m': m, 'ef_construction': ef_construction, 'ef_search': ef_search}\n",
    "}\n",
    "\n",
    "metadata_file = output_dir / \"metadata.pkl\"\n",
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"   Metadata saved to: {metadata_file}\")\n",
    "\n",
    "# 5. Save ground truth (already saved earlier, but confirm)\n",
    "print(\"\\n5. Ground truth already saved to: ground_truth.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL DATA SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - faiss_index_flat.bin\")\n",
    "print(f\"  - faiss_index_ivf.bin\")\n",
    "print(f\"  - faiss_index_hnsw.bin\")\n",
    "print(f\"  - product_ids.pkl\")\n",
    "print(f\"  - products_clean.parquet\")\n",
    "print(f\"  - metadata.pkl\")\n",
    "print(f\"  - ../ground_truth.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Original products: {len(products_df):,}\")\n",
    "print(f\"  Cleaned products: {len(products_df_clean):,}\")\n",
    "print(f\"  Products with embeddings: {flat_index.ntotal:,}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Estimated embedding size: {flat_index.ntotal * embedding_dim * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(f\"\\nIndex Statistics:\")\n",
    "print(f\"  Flat index vectors: {flat_index.ntotal:,}\")\n",
    "print(f\"  IVF index vectors: {ivf_index.ntotal:,} (nlist={nlist}, nprobe=10)\")\n",
    "print(f\"  HNSW index vectors: {hnsw_index.ntotal:,} (m={m})\")\n",
    "\n",
    "print(f\"\\nGround Truth:\")\n",
    "print(f\"  Unique queries: {len(ground_truth):,}\")\n",
    "print(f\"  Average relevant products per query: {np.mean([len(v) for v in ground_truth.values()]):.1f}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  All files saved to: {output_dir}\")\n",
    "print(f\"  Ready for retrieval and evaluation!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA PREPARATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
